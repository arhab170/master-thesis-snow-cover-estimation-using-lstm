{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":640,"status":"ok","timestamp":1700742020971,"user":{"displayName":"Muhammad Arhab","userId":"05674263421856131577"},"user_tz":-330},"id":"0RQVqM81mIdN"},"outputs":[],"source":["about_the_run = \"About the run: Getting the plots\"\n","run, load_run_number = 2, 51             #  0:\"single_run\"  1:\"keras_tuner\" 2:\"load_best_model\"\n","colab = 0\n","model_choice = \"tf\"\n","percentage = 100\n","epochs = 10\n","batch_size=32\n","limit_of_nans_in_a_timestep = 120\n","percentage_of_data_in_summer_months = [0.4]  # Desired number of zero values\n","divide_latitude_in_these_many_parts = 5  #latitude has 10 values\n","divide_longitude_in_these_many_parts = 6 # longitude has 14 values"]},{"cell_type":"markdown","metadata":{},"source":["# Configurations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dMlTqZpimIdM"},"outputs":[],"source":["import xarray as xr\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import r2_score\n","import plotly.express as px\n","import math\n","import time\n","from sklearn.linear_model import LinearRegression\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras import optimizers\n","from kerastuner.tuners import RandomSearch\n","import logging\n","import os\n","import sys\n","from contextlib import contextmanager\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":810,"status":"ok","timestamp":1700741946499,"user":{"displayName":"Muhammad Arhab","userId":"05674263421856131577"},"user_tz":-330},"id":"6-E6nx7mmIdL"},"outputs":[],"source":["# for the WSL conda environment\n","# 1. python kernel is named wslminiconda3 (Python 3.11.14)\n","# 2. the environment is named base and is in the directory /home/arhab/wslminiconda3\n","# 3. Do not use the python kernel named base"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_next_folder_name(base_folder):\n","    run_number = 1\n","    while True:\n","        folder_name = os.path.join(base_folder, f\"run_{run_number}\")\n","        if not os.path.exists(folder_name):\n","            return folder_name\n","        run_number += 1\n","\n","# Base folder directory\n","base_directory = r\"D:\\thesis_data\\notebooks\\model_runs\"\n","\n","# Get the next folder name\n","folder_name = get_next_folder_name(base_directory)\n","\n","# Create the folder\n","os.makedirs(folder_name, exist_ok=True)\n","run_number = f\"run_{folder_name.split('_')[3]}\"\n","print(run_number)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Set up logging and save the log file in the folder\n","log_file = os.path.join(folder_name, 'log.txt')\n","logger = logging.getLogger(folder_name)\n","logger.setLevel(logging.INFO)\n","\n","# Create a file handler and set the formatter\n","file_handler = logging.FileHandler(log_file)\n","formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n","file_handler.setFormatter(formatter)\n","\n","# Add the file handler to the logger\n","logger.addHandler(file_handler)\n","\n","# Create a stream handler to display log messages in Jupyter Notebook console\n","stream_handler = logging.StreamHandler()\n","stream_handler.setFormatter(formatter)\n","logger.addHandler(stream_handler)\n","\n","# Log information\n","logger.info(\"Logging information check\")\n","logger.info(run_number)\n","logger.info(about_the_run)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","logger.info(f\"run = {run}   =>  0:single_run  1:keras_tuner 2:load_best_model\")\n","logger.info(f\"model_choice ={model_choice}\")\n","logger.info(f\"percentage = {percentage}\")\n","logger.info(f\"epochs = {epochs}\")\n","logger.info(f\"batch_size = {batch_size}\")\n","logger.info(f\"limit_of_nans_in_a_timestep = {limit_of_nans_in_a_timestep}\")\n","logger.info(f\"percentage_of_data_in_summer_months = {percentage_of_data_in_summer_months}\")\n","logger.info(f\"divide_latitude_in_these_many_parts = {divide_latitude_in_these_many_parts}\")  \n","logger.info(f\"divide_longitude_in_these_many_parts = {divide_longitude_in_these_many_parts}\") \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import contextlib\n","import sys\n","\n","@contextlib.contextmanager\n","def stdout_redirected(new_stdout):\n","    save_stdout = sys.stdout\n","    sys.stdout = new_stdout\n","    try:\n","        yield None\n","    finally:\n","        sys.stdout = save_stdout\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3683,"status":"ok","timestamp":1700742027235,"user":{"displayName":"Muhammad Arhab","userId":"05674263421856131577"},"user_tz":-330},"id":"0pzFKK9BCrps","outputId":"681f11f6-c223-4966-d2d8-fa9e1dba2db6"},"outputs":[],"source":["if colab == 1:\n","    # !pip install cftime\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    import cftime\n","    %run \"/content/drive/My Drive/Colab Notebooks/main/functions.ipynb\"\n","else: \n","    %run \"functions.ipynb\""]},{"cell_type":"markdown","metadata":{},"source":["# Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":616},"executionInfo":{"elapsed":35,"status":"error","timestamp":1700742185525,"user":{"displayName":"Muhammad Arhab","userId":"05674263421856131577"},"user_tz":-330},"id":"9DZ2cgANmIdP","outputId":"f70c2c04-bf3c-4243-e3e3-20e3c646d7fc"},"outputs":[],"source":["# Step 1: Load NDSI Labels\n","if colab !=1:\n","  ndsi_ds = xr.open_mfdataset(r\".\\cropped_data\\label.nc\")\n","\n","if colab == 1:\n","  ndsi_ds = xr.open_mfdataset(f'/content/drive/My Drive/Colab Notebooks/cropped_data/label.nc')\n","\n","filtered_dates = get_filtered_dates_for_ndsi(limit_of_nans_in_a_timestep= limit_of_nans_in_a_timestep)\n","ndsi_ds = ndsi_ds.sel(time=filtered_dates)\n","selected_dates = get_dates(ndsi_ds)\n","ndsi_labels = ndsi_ds['NDSI_Snow_Cover'].values\n","ndsi_ds = ndsi_ds.interpolate_na(dim='lon', method='linear',  max_gap=4, use_coordinate=False)\n","ndsi_ds = ndsi_ds.interpolate_na(dim='lat', method='linear', max_gap=3, use_coordinate=False)\n","\n","ndsi_ds['time'] = xr.DataArray(ndsi_ds['time'].values.astype('datetime64[ns]'), dims='time', attrs=ndsi_ds['time'].attrs)\n","ndsi_ds.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ndsi_ds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WK0JfX1umIdQ"},"outputs":[],"source":["# Step 2: Load Data\n","# Load climate variables\n","climate_vars = [\"tas\", \"pr\", \"hurs\", \"psl\",  \"rsds\", \"sfcWind\"]\n","data = []\n","\n","for parameter in climate_vars:\n","    scenario = \"observational\"\n","\n","    if colab == 0:\n","      ds = xr.open_mfdataset(rf'.\\cropped_data\\{scenario}\\{parameter}_{scenario}.nc') #fixxx\n","    if colab == 1:\n","      ds = xr.open_mfdataset(f'/content/drive/My Drive/Colab Notebooks/cropped_data/{scenario}/{parameter}_{scenario}.nc')\n","\n","\n","    # ds = ds.sel(time=slice(\"2001-01-01\", \"2018-31-02\"))\n","    \n","    ds = ds.sel(time=selected_dates)\n","    data.append(ds[parameter].values)\n","    ds.close()\n","\n","data = np.array(data)"]},{"cell_type":"markdown","metadata":{},"source":["# Conversion to pandas from xarray"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1700686389572,"user":{"displayName":"Muhammad Arhab","userId":"05674263421856131577"},"user_tz":-330},"id":"vWQYknA-mIdR","outputId":"a023bf2f-959c-430e-c896-664be2cfdeda"},"outputs":[],"source":["# Step 5: Flatten Data\n","n_time_steps = data[0].shape[0]\n","n_lat, n_lon = data[0].shape[1], data[0].shape[2]\n","logger.info(\"for feature\")\n","logger.info(f\"{n_time_steps} , {n_lat}, {n_lon}\")\n","\n","n_time_steps = ndsi_labels.shape[0]\n","n_lat, n_lon = ndsi_labels.shape[1], ndsi_labels.shape[2]\n","logger.info(\"for label\")\n","logger.info(f\"{n_time_steps} , {n_lat}, {n_lon}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7EnP7xTlmIdR"},"outputs":[],"source":["temp0 = to_array(data, 0)\n","temp1 = to_array(data, 1)\n","temp2 = to_array(data, 2)\n","temp3 = to_array(data, 3)\n","temp4 = to_array(data, 4)\n","temp5 = to_array(data, 5)\n","\n","ndsi_array = to_array_ndsi(ndsi_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":969,"status":"ok","timestamp":1700686391732,"user":{"displayName":"Muhammad Arhab","userId":"05674263421856131577"},"user_tz":-330},"id":"6ybL4v8nmIdR","outputId":"179e2569-8f71-4e34-c8a7-89090c70f3b9"},"outputs":[],"source":["dates = ds[\"time\"].values\n","lats = ndsi_ds[\"lat\"].values\n","lons = ndsi_ds[\"lon\"].values\n","\n","#********** Dates *************\n","dates_array_to_append = []\n","for a in range(len(dates)):\n","    x = dates[a]\n","    for b in range(140):\n","        dates_array_to_append.append(x)\n","dates_array_to_append = np.array(dates_array_to_append)\n","\n","#********** Latitude *************\n","lats_array_to_append = []\n","for c in range(len(dates)):\n","    for a in range(len(lats)):\n","        x = lats[a]\n","        for b in range(len(lons)):\n","            lats_array_to_append.append(x)\n","lats_array_to_append = np.array(lats_array_to_append)\n","\n","#********** Longitude *************\n","lons_array_to_append = []\n","for b in range(len(dates)*len(lats)):\n","    for a in range(len(lons)):\n","        x = lons[a]\n","        lons_array_to_append.append(x)\n","\n","lons_array_to_append = np.array(lons_array_to_append)\n","\n","\n","logger.info(f\"Length of Dates: {len(dates_array_to_append)}\")\n","logger.info(f\"Length of Latitude: {len(lats_array_to_append)}\")\n","logger.info(f\"Length of Longitude: {len(lons_array_to_append)}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Data cleaning, feature engineering with pandas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SEdONIZqmIdS"},"outputs":[],"source":["\n","dict_temp = {\n","    \"Date\": dates_array_to_append,\n","    \"Latitude\": lats_array_to_append,\n","    \"Longitude\": lons_array_to_append,\n","    climate_vars[0]: temp0,\n","    climate_vars[1]: temp1,\n","    climate_vars[2]: temp2,\n","    climate_vars[3]: temp3,\n","    climate_vars[4]: temp4,\n","    climate_vars[5]: temp5,\n","    \"ndsi1\": ndsi_array\n","}\n","\n","\n","# Create the DataFrame\n","df = pd.DataFrame(dict_temp)\n","df['Latitude'] = df['Latitude'].round(2)\n","df['Longitude'] = df['Longitude'].round(2)\n","df, drop_nan = df.dropna(ignore_index=False), \"yes\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r_hSMRADmIdS"},"outputs":[],"source":["# df = df.reset_index()\n","df[\"Month\"] = df['Date'].dt.month\n","df['month_sin'] = np.sin(2 * np.pi * df['Month']/12)\n","df['month_cos'] = np.cos(2 * np.pi * df['Month']/12)\n","\n","df['week_number'] = df[\"Date\"].dt.isocalendar().week"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LsgVvTVPmIdS"},"outputs":[],"source":["# # Original one-hot encoding\n","# lat_one_hot_encoded = pd.get_dummies(df['Latitude'], prefix='Latitude', dtype=int)\n","# lon_one_hot_encoded = pd.get_dummies(df['Longitude'], prefix='Longitude', dtype=int)\n","\n","# df = pd.concat([df, lat_one_hot_encoded, lon_one_hot_encoded], axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # One-hot encoding\n","# #Latitude\n","# df['Lat_46_to_46d36'] = (df['Latitude'].between(46, 46.36)).astype(int)\n","# df['Lat_46d37_to_46d69'] = (df['Latitude'].between(46.36, 46.70)).astype(int)\n","# df['Lat_46d8_to_47d14'] = (df['Latitude'].between(46.70, 47.15)).astype(int)\n","\n","# #Longitude\n","# df['Lon_10_to_10d67'] = (df['Longitude'].between(10, 10.67)).astype(int)\n","# df['Lon_10d67_to_11d32'] = (df['Longitude'].between(10.67, 11.32)).astype(int)\n","# df['Lon_11d32_to_11d81'] = (df['Longitude'].between(11.33, 11.82)).astype(int)\n","# df['Lon_11d81_to_12d29'] = (df['Longitude'].between(11.83, 12.29)).astype(int)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lats = np.unique(df.Latitude.values)\n","lons = np.unique(df.Longitude.values)\n","\n","latitude_bins = divide_range(lats[0], lats[-1], divide_latitude_in_these_many_parts)   #10 values \n","longitude_bins = divide_range(lons[0], lons[-1], divide_longitude_in_these_many_parts) #14 values\n","\n","df['Latitude_Group'] = pd.cut(df['Latitude'], bins=latitude_bins, labels=False)\n","df['Longitude_Group'] = pd.cut(df['Longitude'], bins=longitude_bins, labels=False)\n","\n","# Apply one-hot encoding\n","latitude_dummies = pd.get_dummies(df['Latitude_Group'], prefix='Latitude', dtype=int)\n","longitude_dummies = pd.get_dummies(df['Longitude_Group'], prefix='Longitude', dtype=int)\n","\n","# Concatenate one-hot encoded columns with the original DataFrame\n","df = pd.concat([df, latitude_dummies, longitude_dummies], axis=1)\n","temp = df\n","df = df.drop([\"Latitude_Group\",\t\"Longitude_Group\"], axis =1)\n","\n","temp = temp[temp[\"Date\"] == temp.Date[1]]\n","grouped_latitudes = temp.groupby('Latitude_Group')['Latitude'].unique().to_dict()\n","logger.info(\"Latitude Group\")\n","logger.info(\"{\\n\" + \",\\n\".join(f\" {key}: {value}\" for key, value in grouped_latitudes.items()) + \"\\n}\")\n","\n","grouped_longitudes = temp.groupby('Longitude_Group')['Longitude'].unique().to_dict()\n","logger.info(\"Longitude Group\")\n","logger.info(\"{\\n\" + \",\\n\".join(f\" {key}: {value}\" for key, value in grouped_longitudes.items()) + \"\\n}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#bookmark\n","print(df.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[\"ndsi\"] = df[\"ndsi1\"]\n","df= df.drop([\"Month\", \"week_number\", \"ndsi1\"],axis =1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["parameter_array = [\"tas\", \"pr\", \"hurs\", \"psl\",  \"rsds\", \"sfcWind\"]\n","min_max_df = pd.read_csv(\"./min_max_of_all_parameters.csv\")\n","for parameter in parameter_array:\n","    min_val, max_val =  get_min_max(min_max_df, parameter)\n","    df[parameter] = (df[parameter] - min_val) / (max_val - min_val)\n","\n","df['ndsi'] = df['ndsi'].apply(lambda x: x / 100 if not pd.isnull(x) else x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_insurance = df\n","df_insurance"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","df_temp = df_insurance\n","# Plot histogram for df\n","fig1 = px.histogram(df_temp, x='ndsi', nbins=100)\n","fig1.update_layout(\n","    xaxis_title='Value',\n","    yaxis_title='Frequency',\n",")\n","\n","fig1\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# b_array = [0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99]  # Desired number of zero values\n","zero_values_array = []\n","b_array = percentage_of_data_in_summer_months\n","for b in b_array:\n","    \n","    df = df_insurance\n","    df[\"Month\"] = df['Date'].dt.month\n","    df_to_delete = df\n","    df = df[df['Month'].isin([1, 2, 3, 12])]\n","    logger.info(f\"Number of datapoints in Original df: {len(df_to_delete)}\")\n","\n","    # Step 6: Display the final value of b\n","    logger.info(f\"Value of b: {b}\")\n","\n","\n","    # Now you can use this final value of b to sample your DataFrame\n","    final_sampled_data = pd.concat([df_to_delete[(df_to_delete['Date'].dt.month == month)].sample(frac=b) for month in range(4, 12)])\n","    df = pd.concat([final_sampled_data, df], ignore_index=False)\n","    df = df.sort_index()\n","\n","    number_of_zeroes = np.sum((df['ndsi'] >= -0.005) & (df['ndsi'] <= 0.005))\n","    zero_values_array.append(number_of_zeroes)\n","    logger.info(f\"Total number of zeroes: {number_of_zeroes}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # number_of_points_at_the_peak = np.sum((df['ndsi'] >= 0.675) & (df['ndsi'] <= 0.685))\n","# # a = multiplication_factor*number_of_points_at_the_peak - 2500 \n","# a = desired_number_of_zeroes - 2500\n","# tolerance = 0.15 * a \n","\n","# df[\"Month\"] = df['Date'].dt.month\n","# df_to_delete = df\n","# df = df[df['Month'].isin([1, 2, 3, 12])]\n","\n","# logger.info(f\"Tolerance:  {tolerance}\")\n","# logger.info(f\"Final range of near zeores: {a + 2500 - tolerance} - {a + 2500 + tolerance}\")\n","\n","# b = 0.01  # Initial percentage to sample\n","# while True:\n","#     zero_values_array = []\n","#     logger.info(f\"{b}\")\n","    \n","#     # Step 2: Iterate through each month\n","#     for month in range(4, 12):  # Months from April to November\n","#         # Step 3: Sample rows and calculate the number of near zero values\n","#         sampled_data = df_to_delete[(df_to_delete['Date'].dt.month == month)].sample(frac=b)\n","#         zero_values_array.append(np.sum((sampled_data['ndsi'] >= -0.005) & (sampled_data['ndsi'] <= 0.005)))\n","        \n","#     # Step 4: Check the sum of zero values\n","#     total_zero_values = sum(zero_values_array)\n","#     logger.info(f\"{total_zero_values}\")\n","\n","#     # Step 5: Adjust the percentage (b)\n","#     if total_zero_values > a + tolerance:\n","#         b -= 0.01\n","#     elif total_zero_values < a - tolerance:\n","#         b += 0.01\n","#     else:\n","#         break  # Convergence reached\n","\n","# b = round(b,2)\n","# # Step 6: Display the final value of b\n","# logger.info(f\"Final value of b: {b}\")\n","# logger.info(f\"Total zero values: {total_zero_values+ (percentage*2500)}\")\n","\n","# # Now you can use this final value of b to sample your DataFrame\n","# final_sampled_data = pd.concat([df_to_delete[(df_to_delete['Date'].dt.month == month)].sample(frac=b) for month in range(4, 12)])\n","# df = pd.concat([final_sampled_data, df], ignore_index=False)\n","# df = df.sort_index()\n","\n","# # the old code for the sampling has been moved to less_imp/rough"]},{"cell_type":"markdown","metadata":{},"source":["### Plotting of per month data (should be kept folded to keep everything compact)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import plotly.express as px\n","from plotly.subplots import make_subplots\n","import plotly.graph_objs as go\n","\n","# Plot histogram for df\n","fig1 = px.histogram(df, x='ndsi', nbins=100)\n","fig1.update_layout(\n","    xaxis_title='Value',\n","    yaxis_title='Frequency',\n",")\n","\n","# Plot histogram for df_to_delete\n","fig2 = px.histogram(df_to_delete, x='ndsi', nbins=100)\n","fig2.update_layout(\n","    xaxis_title='Value',\n","    yaxis_title='Frequency',\n",")\n","\n","# Create subplots with two columns\n","fig = make_subplots(rows=1, cols=2, subplot_titles=('Before', 'After'))\n","fig.add_trace(fig1.data[0], row=1, col=2)\n","fig.add_trace(fig2.data[0], row=1, col=1)\n","\n","fig.update_layout(\n","    title='Before and after zero sampling in mid-year months',\n","    width=1000,  # Total width of the combined plots\n","    height=400,  # Height of the combined plots\n",")\n","\n","plotly_image_path = os.path.join(folder_name, 'before_after_zero_sampling.png')\n","fig.write_image(plotly_image_path)\n","\n","\n","fig.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#bookmark\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Assuming 'df_to_delete' is your DataFrame\n","# Convert 'Date' column to datetime\n","month_fraction_array_padded = generate_array(b)\n","df_to_delete['Date'] = pd.to_datetime(df_to_delete['Date'])\n","\n","# Extract month from the 'Date' column\n","df_to_delete['Month'] = df_to_delete['Date'].dt.month\n","df['Month'] = df['Date'].dt.month\n","\n","# Create a new figure and subplots\n","fig, axs = plt.subplots(4, 6, figsize=(20, 10))  # 4 rows, 8 columns for 12 months for both DataFrames\n","\n","\n","# row = (month - 1) // 3  # Calculate the row index for the subplot\n","#     col = (month - 1) % 3\n","\n","# Plot histograms for each month in df\n","for month in range(1, 13):\n","    row = (month - 1) // 3  # Calculate the row index for the subplot\n","    col = (month - 1) % 3   # Calculate the column index for the subplot\n","    \n","    # Filter data for the current month in df_to_delete\n","    month_data_df_to_delete = df_to_delete[df_to_delete['Month'] == month]['ndsi']\n","    \n","    # Plot histogram in the corresponding subplot for df_to_delete\n","    axs[row, col].hist(month_data_df_to_delete, bins=20, color='salmon', alpha=0.7)\n","    axs[row, col].set_title(f'Month {month} (before)')\n","    axs[row, col].set_xlabel('NDSI')\n","    axs[row, col].set_ylabel('Frequency')\n","\n","\n","# Plot histograms for each month in df_to_delete on the right\n","for month in range(1, 13):\n","    row = (month - 1) // 3  # Calculate the row index for the subplot\n","    col = (month - 1) % 3 + 3  # Shift to the right by 4 columns\n","    \n","    # Filter data for the current month in df\n","    month_data_df = df[df['Month'] == month]['ndsi']\n","    \n","    # Plot histogram in the corresponding subplot for df\n","    axs[row, col].hist(month_data_df, bins=20, color='skyblue', alpha=0.7)\n","    axs[row, col].set_title(f'Month {month} (after) | Fraction = {month_fraction_array_padded[month-1]}')\n","    axs[row, col].set_xlabel('NDSI')\n","    axs[row, col].set_ylabel('Frequency')\n","    \n","    \n","# # Hide empty subplots\n","# for i in range(12, 16):\n","#     fig.delaxes(axs[3, i])\n","\n","df = df.drop([\"Month\"], axis=1)\n","# Adjust layout and display the subplots\n","plt.tight_layout()\n","\n","matplotlib_image_path = os.path.join(folder_name, 'each_month_zero_cleaning.png')\n","plt.savefig(matplotlib_image_path)\n","\n","\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if percentage == 100:\n","    df.to_csv(\"./complete_df.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["# Data splitting"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47,"status":"ok","timestamp":1700686397725,"user":{"displayName":"Muhammad Arhab","userId":"05674263421856131577"},"user_tz":-330},"id":"MtXlANTgmIdT","outputId":"507d06bf-1d05-442d-f7e4-5c564e04a966"},"outputs":[],"source":["# percentage = 100\n","number_of_rows = int(percentage/100*len(df))\n","logger.info(f\"Number of rows: {number_of_rows}/{len(df)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":543,"status":"ok","timestamp":1700686405589,"user":{"displayName":"Muhammad Arhab","userId":"05674263421856131577"},"user_tz":-330},"id":"A8SgbC3JmIdT","outputId":"3d107652-3e02-44e0-b5e3-5de797c4d2f5"},"outputs":[],"source":["df_reduced = df.sample(frac=percentage/100)\n","df_reduced = df_reduced.sort_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tBK6IUzVmIdT"},"outputs":[],"source":["# test= df_reduced[df_reduced['Date'] >= \"2018-12-30\"]\n","# working_dataset = df_reduced.drop(test.index)\n","# working_dataset = working_dataset.sort_index()\n","\n","working_dataset = df_reduced"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UxtG3w8NmIdT"},"outputs":[],"source":["val = working_dataset.sample(frac=0.2)\n","val = val.sort_index()\n","\n","train = working_dataset.drop(val.index)\n","train = train.sort_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"trNjmOgtmIdT"},"outputs":[],"source":["# X = df.iloc[:number_of_rows, 1:-1].values\n","# y = df.iloc[:number_of_rows, -1].values\n","\n","# Step 7: Split Data\n","X_train, X_val = train.drop([\"Date\", \"ndsi\", \"Latitude\", \"Longitude\"], axis=1).values, val.drop([\"Date\", \"ndsi\", \"Latitude\", \"Longitude\"], axis=1).values\n","y_train, y_val = train[\"ndsi\"].values, val[\"ndsi\"].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GlmYB88FmIdU"},"outputs":[],"source":["# X_test, y_test = test.drop([\"Date\", \"ndsi\",\"Latitude\", \"Longitude\"], axis=1).values, test[\"ndsi\"].values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["temp_shape = X_train.shape[1]\n","# Extracting features and labels\n","X_train = X_train.reshape(-1, 1, temp_shape)  # Reshaping to (146246, 1, 32)\n","y_train = y_train.reshape(-1, 1)  # Assuming the label is in column 35\n","\n","X_val = X_val.reshape(-1, 1, temp_shape)  # Reshaping to (146246, 1, 32)\n","y_val = y_val.reshape(-1, 1)  # Assuming the label is in column 35\n","\n","# X_test = X_test.reshape(-1, 1, temp_shape)  # Reshaping to (146246, 1, 32)\n","# y_test = y_test.reshape(-1, 1)  # Assuming the label is in column 35\n","\n","\n","# logger.infoing shapes\n","logger.info(f\"y_train shape is: {y_train.shape}\")\n","logger.info(f\"X_train shape is: {X_train.shape}\")\n","\n","logger.info(f\"y_val shape is: {y_val.shape}\")\n","logger.info(f\"X_val shape is: {X_val.shape}\")\n","\n","# logger.info(f\"y_test shape is: {y_test.shape}\")\n","# logger.info(f\"X_test shape is: {X_test.shape}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model Architecture"]},{"cell_type":"markdown","metadata":{},"source":["## Machine learning stuff, mainly folded"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dddH0F-omIdU"},"outputs":[],"source":["if model_choice==\"ml\":\n","    # Step 8: Train Random Forest Model\n","    rf_model = RandomForestRegressor(n_estimators=100)\n","\n","    start_time = time.time()\n","    rf_model.fit(X_train, y_train)\n","    end_time = time.time()\n","    execution_time_fitting = end_time - start_time\n","    logger.info(f\"Time taken to fit {execution_time_fitting/60} min\")\n","    logger.info(\"Fitting done\")\n","\n","    # Step 9: Model historical\n","    start_time = time.time()\n","    y_val_pred = rf_model.predict(X_val)\n","    end_time = time.time()\n","    # y_test_pred = rf_model.predict(X_test)\n","    execution_time_prediciting = end_time - start_time\n","\n","    logger.info(f\"Time taken to predict {execution_time_prediciting}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Deep learning stuff, the main thing "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":995658,"status":"ok","timestamp":1700687406667,"user":{"displayName":"Muhammad Arhab","userId":"05674263421856131577"},"user_tz":-330},"id":"5uUDTw6PmIdU","outputId":"364413df-9e4a-4d9f-c3e9-4b486090f92a"},"outputs":[],"source":["if model_choice==\"tf\":\n","    import tensorflow as tf\n","    from tensorflow.keras.models import Sequential\n","    from tensorflow.keras.layers import Dense, Dropout, LSTM, SimpleRNN\n","    from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","    from tensorflow.keras import optimizers\n","\n","    # Check for GPU availability\n","    if tf.config.list_physical_devices('GPU'):\n","        logger.info('GPU found. Running on GPU.')\n","    else:\n","        logger.info('No GPU found. Running on CPU.')\n","\n","    # Specify GPU device if available\n","    physical_devices = tf.config.list_physical_devices('GPU')\n","    if len(physical_devices) > 0:\n","        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n","\n","    # Convert data to TensorFlow tensors\n","    X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n","    y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n","    \n","    X_val_tensor = tf.convert_to_tensor(X_val, dtype=tf.float32)\n","    y_val_tensor = tf.convert_to_tensor(y_val, dtype=tf.float32)\n","    \n","    # X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)\n","    # y_test_tensor = tf.convert_to_tensor(y_test, dtype=tf.float32)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if model_choice==\"tf\" and run ==0:\n","    # Define the model\n","    model = Sequential()\n","    model.add(LSTM(units=180, input_shape=(1,temp_shape), return_sequences=True, activation='relu'))\n","    model.add(LSTM(50, return_sequences=True))\n","    model.add(LSTM(10, return_sequences=True))\n","    model.add(LSTM(5, return_sequences=False))\n","    model.add(Dense(10, activation='relu'))\n","    model.add(Dense(1, activation='linear'))  # Assuming NDSI is a continuous variable\n","\n","    # Compile the model\n","    model.compile(optimizer=\"adam\", loss='mean_squared_error', metrics=['mae'])\n","\n","    model_save_path = os.path.join(folder_name, 'model.h5')\n","    callbacks = [\n","        EarlyStopping(monitor='val_loss', patience=4),\n","        ModelCheckpoint(model_save_path, monitor='val_loss', save_best_only=True, mode='min')\n","    ]\n","\n","    with open(log_file, 'a') as f:\n","        with stdout_redirected(f):\n","            model.summary()\n","            \n","            # Train the model on GPU\n","            start_time = time.time()\n","            history = model.fit(X_train_tensor, y_train_tensor, epochs=epochs, batch_size=batch_size, callbacks=callbacks, validation_data=(X_val_tensor, y_val_tensor))\n","            end_time = time.time()\n","            execution_time_fitting = end_time - start_time\n","\n","            # Evaluate the model on GPU\n","            loss, mae = model.evaluate(X_val_tensor, y_val_tensor)\n","            logger.info(f'Mean absolute Error on val Set: {mae}')\n","\n","    # Make predictions on GPU\n","    y_val_pred = model.predict(X_val_tensor)\n","    # y_test_pred = model.predict(X_test_tensor)\n","\n","    remove_eta_lines(run_number)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError, Huber\n","if model_choice == \"tf\" and run == 1:\n","\n","    # Define a function to build the model with hyperparameters\n","    def build_model(hp):\n","        model = Sequential()\n","        model.add(LSTM(units=hp.Int('units_1', min_value=50, max_value=200, step=10), input_shape=(1, temp_shape), return_sequences=True, activation=hp.Choice('activation_1', ['sigmoid', 'tanh', 'relu'])))\n","        model.add(LSTM(units=hp.Int('units_2', min_value=20, max_value=100, step=10), return_sequences=True, activation=hp.Choice('activation_2', ['sigmoid', 'tanh', 'relu'])))\n","        model.add(LSTM(units=hp.Int('units_3', min_value=10, max_value=50, step=10), return_sequences=True, activation=hp.Choice('activation_3', ['sigmoid', 'tanh', 'relu'])))\n","        model.add(LSTM(units=hp.Int('units_4', min_value=5, max_value=30, step=5), return_sequences=False, activation=hp.Choice('activation_4', ['sigmoid', 'tanh', 'relu'])))\n","        model.add(Dense(units=hp.Int('units_5', min_value=10, max_value=100, step=10), activation='relu'))\n","        model.add(Dense(1, activation='linear'))\n","\n","        optimizer_choice = hp.Choice('optimizer', ['adam', 'rmsprop', 'sgd'])  # Define optimizers for regression\n","        if optimizer_choice == 'adam':\n","            optimizer = optimizers.Adam(learning_rate=hp.Choice('learning_rate_adam', values=[1e-2, 1e-3, 1e-4]))\n","        elif optimizer_choice == 'rmsprop':\n","            optimizer = optimizers.RMSprop(learning_rate=hp.Choice('learning_rate_rmsprop', values=[1e-2, 1e-3, 1e-4]))\n","        else:\n","            optimizer = optimizers.SGD(learning_rate=hp.Choice('learning_rate_sgd', values=[1e-2, 1e-3, 1e-4]))\n","\n","        loss_choice = hp.Choice('loss', ['mse', 'mae', 'huber_loss'])  # Define loss functions\n","        if loss_choice == 'mse':\n","            loss = MeanSquaredError()\n","        elif loss_choice == 'mae':\n","            loss = MeanAbsoluteError()\n","        else:\n","            loss = Huber()\n","\n","        model.compile(optimizer=optimizer, loss=loss, metrics=['mae'])\n","        return model\n","\n","    # Instantiate the tuner for hyperparameter search\n","    tuner = RandomSearch(\n","        build_model,\n","        objective='val_loss',\n","        max_trials=100,  # Number of hyperparameter combinations to try\n","        executions_per_trial=1,\n","        directory=folder_name,  # Directory to store the results\n","        project_name='regression_optimizers_tuner_2'\n","    )\n","\n","    with open(log_file, 'a') as f:\n","        with stdout_redirected(f):\n","            # Search for the best hyperparameter configuration\n","            tuner.search(X_train_tensor, y_train_tensor, epochs=10, batch_size = 128, validation_data=(X_val_tensor, y_val_tensor))\n","\n","    remove_eta_lines(run_number)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if model_choice==\"tf\" and run ==1:\n","    # Get the best hyperparameters and build the final model\n","    best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n","    model = tuner.hypermodel.build(best_hp)\n","\n","    # Train the best model\n","    history = model.fit(X_train_tensor, y_train_tensor, epochs=epochs, batch_size=batch_size, validation_data=(X_val_tensor, y_val_tensor), callbacks=[EarlyStopping(patience=20)])\n","\n","    # Evaluate the best model\n","    loss, mae = model.evaluate(X_val_tensor, y_val_tensor)\n","    logger.info(f'Mean absolute Error on val Set: {mae}')\n","\n","    # Make predictions using the best model\n","    y_val_pred = model.predict(X_val_tensor)\n","    # y_test_pred = best_model.predict(X_test_tensor)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# run =2\n","# load_run_number = 51"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from keras.models import load_model\n","if model_choice==\"tf\" and run ==2:\n","    # Provide the directory path where checkpoint files or model information is stored\n","\n","    # Load the model\n","    model = load_model(rf'D:\\thesis_data\\notebooks\\model_runs\\run_{load_run_number}\\model.h5')\n","\n","    # Evaluate the best model\n","    loss, mae = model.evaluate(X_val_tensor, y_val_tensor)\n","    logger.info(f'Mean absolute Error on val Set: {mae}')\n","\n","    # Make predictions using the best model\n","    y_val_pred = model.predict(X_val_tensor)\n","    # y_test_pred = loaded_model.predict(X_test_tensor)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"onv4DFogEnhl"},"outputs":[],"source":["if model_choice==\"tf\" and run!=2:\n","    # Plot the training and validation loss\n","    plt.plot(history.history['loss'], label='Training Loss')\n","    plt.plot(history.history['val_loss'], label='Validation Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Mean Squared Error')\n","    plt.legend()\n","    \n","    matplotlib_image_path = os.path.join(folder_name, 'loss_vs_epoch_graph.png')\n","    plt.savefig(matplotlib_image_path)\n","    \n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YR9TjTZ3mIdV"},"outputs":[],"source":["import math\n","from sklearn.metrics import  mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import r2_score\n","\n","if model_choice == \"tf\":\n","    y_val_pred = y_val_pred.flatten()\n","    y_val = y_val.flatten()\n","\n","\n","mae = mean_absolute_error(y_val, y_val_pred)\n","r2 = r2_score(y_val, y_val_pred)\n","# nse_value = nse(y_val_pred, y_val)\n","\n","mse_value = mean_squared_error(y_val,y_val_pred)\n","rmse_value = math.sqrt(mse_value)\n","# n_rmse_value = calculate_n_rmse(y_val, y_val_pred)\n","\n","pbias_value = pbias(y_val, y_val_pred)\n","\n","temp_percentage = round(number_of_rows*100/len(df),2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wjd1nuVHmIdV","outputId":"72861d07-0b91-46dd-dd92-1b16af33aa17"},"outputs":[],"source":["logger.info(f\"{number_of_rows}/{len(df)} = {temp_percentage} % \\n\")\n","# logger.info(f\"Time taken to fit {execution_time_fitting/60} min\")\n","# logger.info(f\"NSE value: {nse_value}\")\n","logger.info(f'R2 Score: {r2}')\n","# logger.info(f'Self defined R2 Score: {self_defined_r2} \\n')\n","logger.info(f\"P-bias value: {pbias_value} \\n\")\n","\n","# logger.info(f\"MSE value: {mse_value}\")\n","logger.info(f\"RMSE value: {rmse_value}\")\n","# logger.info(f\"N-RMSE value: {n_rmse_value}\")"]},{"cell_type":"markdown","metadata":{"notebookRunGroups":{"groupValue":"1"}},"source":["# Plotting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZXAMMobTmIdV","outputId":"b90b9f7b-acad-469b-f15f-2016dbe59e05"},"outputs":[],"source":["# y_val_pred = y_val_pred.flatten()\n","comparison_df = pd.DataFrame({\n","    \"Date\" : val[\"Date\"].values,\n","    \"Latitude\" : val[\"Latitude\"].values,\n","    \"Longitude\" : val[\"Longitude\"].values,\n","    \"Observed\": y_val,\n","    \"Predicted\": y_val_pred.flatten()\n","    })\n","comparison_df['Date'] = pd.to_datetime(comparison_df['Date'])\n","\n","df_image_path = os.path.join(folder_name, 'val_vs_predicted.csv')\n","comparison_df.to_csv(df_image_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["comparison_df, scale = comparison_df.sort_values(by='Date'), \"Original without mean\"\n","fig = px.scatter(comparison_df, x=\"Date\" , y=[\"Predicted\",\"Observed\"])\n","fig.update_layout(title=f'Predicted vs Observed | {scale}', width = 1800, height = 400)\n","\n","plotly_image_path = os.path.join(folder_name, 'Original_without_mean.png')\n","fig.write_image(plotly_image_path)\n","\n","fig"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["comparison_df_1 = comparison_df[(comparison_df['Latitude'] ==  46.58) & (comparison_df['Longitude'] == 11.15)]\n","comparison_df_1, scale = comparison_df_1.sort_values(by='Date'), \"Centre without mean\"\n","fig = px.line(comparison_df_1, x=\"Date\" , y=[\"Predicted\",\"Observed\"])\n","fig.update_layout(title=f'Predicted vs Observed | {scale}', width = 1800, height = 400)\n","\n","plotly_image_path = os.path.join(folder_name, 'Centre_without_mean.png')\n","fig.write_image(plotly_image_path)\n","\n","fig"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# top left\n","temp_lat =  lats[-3]\n","temp_lon = lons[2]\n","comparison_df_1 = comparison_df[(comparison_df['Latitude'] ==  temp_lat) & (comparison_df['Longitude'] == temp_lon)]\n","comparison_df_1, scale = comparison_df_1.sort_values(by='Date'), \"Top left\"\n","fig = px.line(comparison_df_1, x=\"Date\" , y = [\"Predicted\",\"Observed\"])\n","fig.update_layout(title=f' | {scale}', width = 1800, height = 400)\n","\n","plotly_image_path = os.path.join(folder_name, f\"{scale}.png\")\n","fig.write_image(plotly_image_path)\n","\n","fig"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# top right\n","temp_lat =  lats[-2]\n","temp_lon = lons[-2]\n","comparison_df_1 = comparison_df[(comparison_df['Latitude'] ==  temp_lat) & (comparison_df['Longitude'] == temp_lon)]\n","comparison_df_1, scale = comparison_df_1.sort_values(by='Date'), \"Top Right\"\n","fig = px.line(comparison_df_1, x=\"Date\" , y = [\"Predicted\",\"Observed\"])\n","fig.update_layout(title=f'Predicted vs Observed| {scale}', width = 1800, height = 400)\n","\n","plotly_image_path = os.path.join(folder_name, f\"{scale}.png\")\n","fig.write_image(plotly_image_path)\n","\n","fig"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# bottom left\n","temp_lat =  lats[1]\n","temp_lon = lons[2]\n","comparison_df_1 = comparison_df[(comparison_df['Latitude'] ==  temp_lat) & (comparison_df['Longitude'] == temp_lon)]\n","comparison_df_1, scale = comparison_df_1.sort_values(by='Date'), \"Bottom left\"\n","fig = px.line(comparison_df_1, x=\"Date\" ,y=[\"Predicted\",\"Observed\"])\n","fig.update_layout(title=f'Predicted vs Observed| {scale}', width = 1800, height = 400)\n","\n","plotly_image_path = os.path.join(folder_name, f\"{scale}.png\")\n","fig.write_image(plotly_image_path)\n","\n","fig"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# bottom right\n","temp_lat =  lats[1]\n","temp_lon = lons[-4]\n","comparison_df_1 = comparison_df[(comparison_df['Latitude'] ==  temp_lat) & (comparison_df['Longitude'] == temp_lon)]\n","comparison_df_1, scale = comparison_df_1.sort_values(by='Date'), \"Bottom right\"\n","fig = px.line(comparison_df_1, x=\"Date\" , y=[\"Predicted\",\"Observed\"])\n","fig.update_layout(title=f'Predicted vs Observed | {scale}', width = 1800, height = 400)\n","\n","plotly_image_path = os.path.join(folder_name, f\"{scale}.png\")\n","fig.write_image(plotly_image_path)\n","\n","fig"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["comparison_df['Date'], scale = comparison_df['Date'].dt.strftime('%d-%m-%Y'), \"Average Daily\"\n","comparison_df= comparison_df.groupby('Date')[[\"Observed\", \"Predicted\"]].mean().reset_index()\n","comparison_df['Date'] = pd.to_datetime(comparison_df['Date'], format='%d-%m-%Y')\n","comparison_df = comparison_df.sort_values(by='Date')\n","# comparison_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = px.scatter(comparison_df, x=\"Date\" , y=[\"Predicted\",\"Observed\"])\n","fig.update_layout(title=f'Predicted vs Observed | {scale}', width = 1800, height = 400)\n","\n","plotly_image_path = os.path.join(folder_name, 'Average daily.png')\n","fig.write_image(plotly_image_path)\n","fig"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = px.line(comparison_df, x=\"Date\" , y=[\"Predicted\",\"Observed\"])\n","fig.update_layout(title=f'Predicted vs Observed | {scale}', width = 1800, height = 400)\n","\n","plotly_image_path = os.path.join(folder_name, 'Average daily (line).png')\n","fig.write_image(plotly_image_path)\n","fig"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["comparison_df['Date'], scale = comparison_df['Date'].dt.strftime('%m-%Y'), \"Average monthly\"\n","comparison_df= comparison_df.groupby('Date')[[\"Observed\", \"Predicted\"]].mean().reset_index()\n","comparison_df['Date'] = pd.to_datetime(comparison_df['Date'])\n","comparison_df = comparison_df.sort_values(by='Date')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n3TheIJxmIdV","outputId":"4d72860f-2b71-428a-8581-fd4ddcdeee1e"},"outputs":[],"source":["fig = px.line(comparison_df, x=\"Date\" , y=[\"Predicted\",\"Observed\"])\n","fig.update_layout(title=f'Predicted vs Observed | {scale}', width = 1800, height = 400)\n","\n","\n","plotly_image_path = os.path.join(folder_name, 'Average monthly.png')\n","fig.write_image(plotly_image_path)\n","\n","fig"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"xarray-tutorial","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
