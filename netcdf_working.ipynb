{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":640,"status":"ok","timestamp":1700742020971,"user":{"displayName":"Muhammad Arhab","userId":"05674263421856131577"},"user_tz":-330},"id":"0RQVqM81mIdN"},"outputs":[],"source":["parameter = \"pr\"\n","# \"tas\", \"pr\", \"huss\", \"rlds\",  \"rsds\", \"sfcwind\"\n","\n","run, load_run_number = 0, 7             #  0:\"single_run\"  1:\"keras_tuner\" 2:\"load_best_model\"\n","scenario_option = 0\n","label = 0\n","colab = 0\n","model_choice = \"tf\"\n","percentage = 1\n","epochs = 2\n","batch_size=32\n","percentage_limit=0.35\n","pixel_interpolation_limit=50\n","# desired_zero_array = []\n","divide_latitude_in_these_many_parts = 5  #latitude has 10 values\n","divide_longitude_in_these_many_parts = 6 # longitude has 14 values\n","\n","\n","if scenario_option == 0:\n","    scenario = \"historical\"\n","elif scenario_option ==1:\n","    scenario = \"rcp26\"\n","elif scenario_option ==2:\n","    scenario = \"rcp45\"\n","elif scenario_option ==3:\n","    scenario = \"rcp85\"\n"]},{"cell_type":"markdown","metadata":{},"source":["# Configurations"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"dMlTqZpimIdM"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\suvam\\AppData\\Local\\Temp\\ipykernel_10588\\3557830353.py:15: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n","  from kerastuner.tuners import RandomSearch\n"]}],"source":["import xarray as xr\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import r2_score\n","import plotly.express as px\n","import math\n","import time\n","from sklearn.linear_model import LinearRegression\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras import optimizers\n","from kerastuner.tuners import RandomSearch\n","import logging\n","import os\n","import sys\n","from contextlib import contextmanager\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":810,"status":"ok","timestamp":1700741946499,"user":{"displayName":"Muhammad Arhab","userId":"05674263421856131577"},"user_tz":-330},"id":"6-E6nx7mmIdL"},"outputs":[],"source":["# for the WSL conda environment\n","# 1. python kernel is named wslminiconda3 (Python 3.11.14)\n","# 2. the environment is named base and is in the directory /home/arhab/wslminiconda3\n","# 3. Do not use the python kernel named base"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["run_27\n"]}],"source":["def get_next_folder_name(base_folder):\n","    run_number = 1\n","    while True:\n","        folder_name = os.path.join(base_folder, f\"run_{run_number}\")\n","        if not os.path.exists(folder_name):\n","            return folder_name\n","        run_number += 1\n","\n","# Base folder directory\n","base_directory = r\"D:\\thesis_data\\notebooks\\model_runs\"\n","\n","# Get the next folder name\n","folder_name = get_next_folder_name(base_directory)\n","\n","# Create the folder\n","os.makedirs(folder_name, exist_ok=True)\n","run_number = f\"run_{folder_name.split('_')[3]}\"\n","print(run_number)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-01-18 13:00:25,255 - INFO - Logging information check\n","2024-01-18 13:00:25,257 - INFO - run_27\n"]}],"source":["# Set up logging and save the log file in the folder\n","log_file = os.path.join(folder_name, 'log.txt')\n","logger = logging.getLogger(folder_name)\n","logger.setLevel(logging.INFO)\n","\n","# Create a file handler and set the formatter\n","file_handler = logging.FileHandler(log_file)\n","formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n","file_handler.setFormatter(formatter)\n","\n","# Add the file handler to the logger\n","logger.addHandler(file_handler)\n","\n","# Create a stream handler to display log messages in Jupyter Notebook console\n","stream_handler = logging.StreamHandler()\n","stream_handler.setFormatter(formatter)\n","logger.addHandler(stream_handler)\n","\n","# Log information\n","logger.info(\"Logging information check\")\n","logger.info(run_number)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-01-18 13:00:25,270 - INFO - run = 0   =>  0:single_run  1:keras_tuner 2:load_best_model\n","2024-01-18 13:00:25,271 - INFO - model_choice =tf\n","2024-01-18 13:00:25,273 - INFO - percentage = 1\n","2024-01-18 13:00:25,275 - INFO - epochs = 2\n","2024-01-18 13:00:25,277 - INFO - batch_size = 32\n","2024-01-18 13:00:25,278 - INFO - percentage_limit = 0.35\n","2024-01-18 13:00:25,280 - INFO - pixel_interpolation_limit = 50\n"]}],"source":["logger.info(f\"run = {run}   =>  0:single_run  1:keras_tuner 2:load_best_model\")\n","logger.info(f\"model_choice ={model_choice}\")\n","logger.info(f\"percentage = {percentage}\")\n","logger.info(f\"epochs = {epochs}\")\n","logger.info(f\"batch_size = {batch_size}\")\n","logger.info(f\"percentage_limit = {percentage_limit}\")\n","logger.info(f\"pixel_interpolation_limit = {pixel_interpolation_limit}\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import contextlib\n","import sys\n","\n","@contextlib.contextmanager\n","def stdout_redirected(new_stdout):\n","    save_stdout = sys.stdout\n","    sys.stdout = new_stdout\n","    try:\n","        yield None\n","    finally:\n","        sys.stdout = save_stdout\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3683,"status":"ok","timestamp":1700742027235,"user":{"displayName":"Muhammad Arhab","userId":"05674263421856131577"},"user_tz":-330},"id":"0pzFKK9BCrps","outputId":"681f11f6-c223-4966-d2d8-fa9e1dba2db6"},"outputs":[],"source":["if colab == 1:\n","    # !pip install cftime\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    import cftime\n","    %run \"/content/drive/My Drive/Colab Notebooks/main/functions.ipynb\"\n","else: \n","    %run \"functions.ipynb\""]},{"cell_type":"markdown","metadata":{},"source":["# Data Loading"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":616},"executionInfo":{"elapsed":35,"status":"error","timestamp":1700742185525,"user":{"displayName":"Muhammad Arhab","userId":"05674263421856131577"},"user_tz":-330},"id":"9DZ2cgANmIdP","outputId":"f70c2c04-bf3c-4243-e3e3-20e3c646d7fc"},"outputs":[],"source":["# Step 1: Load NDSI Labels\n","if colab !=1:\n","  ndsi_ds = xr.open_mfdataset(r\"D:\\thesis_data\\new_cordex_data\\cropped_data\\label.nc\")\n","\n","if colab == 1:\n","  ndsi_ds = xr.open_mfdataset(f'/content/drive/My Drive/Colab Notebooks/cropped_data/label.nc')\n","\n","# ndsi_ds = ndsi_ds.sel(time=slice(\"2001-01-01\", \"2001-03-02\"))\n","filtered_dates, nan_interpolation_df = get_filtered_dates_for_ndsi(percentage_limit=percentage_limit, pixel_interpolation_limit=pixel_interpolation_limit)\n","ndsi_ds = ndsi_ds.sel(time=filtered_dates)\n","selected_dates = get_dates(ndsi_ds)\n","ndsi_labels = ndsi_ds['NDSI_Snow_Cover'].values\n","ndsi_ds = ndsi_ds.interpolate_na(dim='lon', method='linear',  max_gap=8, use_coordinate=False)\n","ndsi_ds = ndsi_ds.interpolate_na(dim='lat', method='linear', max_gap=5, use_coordinate=False)\n","\n","ndsi_ds['time'] = xr.DataArray(ndsi_ds['time'].values.astype('datetime64[ns]'), dims='time', attrs=ndsi_ds['time'].attrs)\n","ndsi_ds.close()"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"WK0JfX1umIdQ"},"outputs":[],"source":["# Step 2: Load Data\n","# Load climate variables\n","climate_vars = [\"tas\", \"pr\", \"huss\", \"rlds\",  \"rsds\", \"sfcWind\"]\n","data = []\n","\n","for parameter in climate_vars:\n","    scenario = \"historical\"\n","\n","    if colab == 0:\n","      ds = xr.open_mfdataset(rf'D:\\thesis_data\\new_cordex_data\\cropped_data\\historical\\{parameter}_historical.nc') #fixxx\n","    if colab == 1:\n","      ds = xr.open_mfdataset(f'/content/drive/My Drive/Colab Notebooks/cropped_data/historical/{parameter}_{scenario}.nc')\n","\n","\n","    # ds = ds.sel(time=slice(\"2001-01-01\", \"2018-31-02\"))\n","    ds = ds.sel(time=selected_dates)\n","    data.append(ds[parameter].values)\n","    ds.close()\n","\n","data = np.array(data)"]},{"cell_type":"markdown","metadata":{},"source":["# Conversion to pandas from xarray"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1700686389572,"user":{"displayName":"Muhammad Arhab","userId":"05674263421856131577"},"user_tz":-330},"id":"vWQYknA-mIdR","outputId":"a023bf2f-959c-430e-c896-664be2cfdeda"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-01-18 13:00:28,165 - INFO - for feature\n","2024-01-18 13:00:28,166 - INFO - 3474 , 10, 14\n","2024-01-18 13:00:28,167 - INFO - for label\n","2024-01-18 13:00:28,169 - INFO - 3474 , 10, 14\n"]}],"source":["# Step 5: Flatten Data\n","n_time_steps = data[0].shape[0]\n","n_lat, n_lon = data[0].shape[1], data[0].shape[2]\n","logger.info(\"for feature\")\n","logger.info(f\"{n_time_steps} , {n_lat}, {n_lon}\")\n","\n","n_time_steps = ndsi_labels.shape[0]\n","n_lat, n_lon = ndsi_labels.shape[1], ndsi_labels.shape[2]\n","logger.info(\"for label\")\n","logger.info(f\"{n_time_steps} , {n_lat}, {n_lon}\")\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"7EnP7xTlmIdR"},"outputs":[],"source":["temp0 = to_array(data, 0)\n","temp1 = to_array(data, 1)\n","temp2 = to_array(data, 2)\n","temp3 = to_array(data, 3)\n","temp4 = to_array(data, 4)\n","temp5 = to_array(data, 5)\n","\n","ndsi_array = to_array_ndsi(ndsi_labels)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":969,"status":"ok","timestamp":1700686391732,"user":{"displayName":"Muhammad Arhab","userId":"05674263421856131577"},"user_tz":-330},"id":"6ybL4v8nmIdR","outputId":"179e2569-8f71-4e34-c8a7-89090c70f3b9"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-01-18 13:00:30,640 - INFO - Length of Dates: 486360\n","2024-01-18 13:00:30,652 - INFO - Length of Latitude: 486360\n","2024-01-18 13:00:30,653 - INFO - Length of Longitude: 486360\n"]}],"source":["dates = ds[\"time\"].values\n","lats = ndsi_ds[\"lat\"].values\n","lons = ndsi_ds[\"lon\"].values\n","\n","#********** Dates *************\n","dates_array_to_append = []\n","for a in range(len(dates)):\n","    x = dates[a]\n","    for b in range(140):\n","        dates_array_to_append.append(x)\n","dates_array_to_append = np.array(dates_array_to_append)\n","\n","#********** Latitude *************\n","lats_array_to_append = []\n","for c in range(len(dates)):\n","    for a in range(len(lats)):\n","        x = lats[a]\n","        for b in range(len(lons)):\n","            lats_array_to_append.append(x)\n","lats_array_to_append = np.array(lats_array_to_append)\n","\n","#********** Longitude *************\n","lons_array_to_append = []\n","for b in range(len(dates)*len(lats)):\n","    for a in range(len(lons)):\n","        x = lons[a]\n","        lons_array_to_append.append(x)\n","\n","lons_array_to_append = np.array(lons_array_to_append)\n","\n","\n","logger.info(f\"Length of Dates: {len(dates_array_to_append)}\")\n","logger.info(f\"Length of Latitude: {len(lats_array_to_append)}\")\n","logger.info(f\"Length of Longitude: {len(lons_array_to_append)}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Data cleaning, feature engineering with pandas"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"SEdONIZqmIdS"},"outputs":[],"source":["dict_temp = {\n","    \"Date\": dates_array_to_append,\n","    \"Latitude\": lats_array_to_append,\n","    \"Longitude\": lons_array_to_append,\n","    climate_vars[0]: temp0,\n","    climate_vars[1]: temp1,\n","    climate_vars[2]: temp2,\n","    climate_vars[3]: temp3,\n","    climate_vars[4]: temp4,\n","    climate_vars[5]: temp5,\n","    \"ndsi1\": ndsi_array\n","}\n","\n","\n","# Create the DataFrame\n","df = pd.DataFrame(dict_temp)\n","df['Latitude'] = df['Latitude'].round(2)\n","df['Longitude'] = df['Longitude'].round(2)\n","df, drop_nan = df.dropna(ignore_index=False), \"yes\"\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"r_hSMRADmIdS"},"outputs":[],"source":["# df = df.reset_index()\n","df[\"Month\"] = df['Date'].dt.month\n","df['month_sin'] = np.sin(2 * np.pi * df['Month']/12)\n","df['month_cos'] = np.cos(2 * np.pi * df['Month']/12)\n","\n","df['week_number'] = df[\"Date\"].dt.isocalendar().week"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"LsgVvTVPmIdS"},"outputs":[],"source":["# Original one-hot encoding\n","# lat_one_hot_encoded = pd.get_dummies(df['Latitude'], prefix='Latitude', dtype=int)\n","# lon_one_hot_encoded = pd.get_dummies(df['Longitude'], prefix='Longitude', dtype=int)\n","\n","# df = pd.concat([df, lat_one_hot_encoded, lon_one_hot_encoded], axis=1)\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# # One-hot encoding\n","# #Latitude\n","# df['Lat_46_to_46d36'] = (df['Latitude'].between(46, 46.36)).astype(int)\n","# df['Lat_46d37_to_46d69'] = (df['Latitude'].between(46.36, 46.70)).astype(int)\n","# df['Lat_46d8_to_47d14'] = (df['Latitude'].between(46.70, 47.15)).astype(int)\n","\n","# #Longitude\n","# df['Lon_10_to_10d67'] = (df['Longitude'].between(10, 10.67)).astype(int)\n","# df['Lon_10d67_to_11d32'] = (df['Longitude'].between(10.67, 11.32)).astype(int)\n","# df['Lon_11d32_to_11d81'] = (df['Longitude'].between(11.33, 11.82)).astype(int)\n","# df['Lon_11d81_to_12d29'] = (df['Longitude'].between(11.83, 12.29)).astype(int)\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-01-18 13:00:31,149 - INFO - Latitude Group\n","2024-01-18 13:00:31,151 - INFO - {\n"," 0: [46.13 46.24],\n"," 1: [46.35 46.47],\n"," 2: [46.58 46.69],\n"," 3: [46.8  46.92],\n"," 4: [47.03 47.14]\n","}\n","2024-01-18 13:00:31,155 - INFO - Longitude Group\n","2024-01-18 13:00:31,158 - INFO - {\n"," 0: [10.18 10.34 10.5 ],\n"," 1: [10.66 10.82],\n"," 2: [10.99 11.15],\n"," 3: [11.31 11.47],\n"," 4: [11.64 11.8 ],\n"," 5: [11.96 12.12 12.28]\n","}\n"]}],"source":["lats = np.unique(df.Latitude.values)\n","lons = np.unique(df.Longitude.values)\n","\n","latitude_bins = divide_range(lats[0], lats[-1], divide_latitude_in_these_many_parts)   #10 values \n","longitude_bins = divide_range(lons[0], lons[-1], divide_longitude_in_these_many_parts) #14 values\n","\n","df['Latitude_Group'] = pd.cut(df['Latitude'], bins=latitude_bins, labels=False)\n","df['Longitude_Group'] = pd.cut(df['Longitude'], bins=longitude_bins, labels=False)\n","\n","# Apply one-hot encoding\n","latitude_dummies = pd.get_dummies(df['Latitude_Group'], prefix='Latitude', dtype=int)\n","longitude_dummies = pd.get_dummies(df['Longitude_Group'], prefix='Longitude', dtype=int)\n","\n","# Concatenate one-hot encoded columns with the original DataFrame\n","df = pd.concat([df, latitude_dummies, longitude_dummies], axis=1)\n","temp = df\n","df = df.drop([\"Latitude_Group\",\t\"Longitude_Group\"], axis =1)\n","\n","temp = temp[temp[\"Date\"] == temp.Date[1]]\n","grouped_latitudes = temp.groupby('Latitude_Group')['Latitude'].unique().to_dict()\n","logger.info(\"Latitude Group\")\n","logger.info(\"{\\n\" + \",\\n\".join(f\" {key}: {value}\" for key, value in grouped_latitudes.items()) + \"\\n}\")\n","\n","grouped_longitudes = temp.groupby('Longitude_Group')['Longitude'].unique().to_dict()\n","logger.info(\"Longitude Group\")\n","logger.info(\"{\\n\" + \",\\n\".join(f\" {key}: {value}\" for key, value in grouped_longitudes.items()) + \"\\n}\")\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['Date', 'Latitude', 'Longitude', 'tas', 'pr', 'huss', 'rlds', 'rsds',\n","       'sfcWind', 'ndsi1', 'Month', 'month_sin', 'month_cos', 'week_number',\n","       'Latitude_0', 'Latitude_1', 'Latitude_2', 'Latitude_3', 'Latitude_4',\n","       'Longitude_0', 'Longitude_1', 'Longitude_2', 'Longitude_3',\n","       'Longitude_4', 'Longitude_5'],\n","      dtype='object')\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Date</th>\n","      <th>Latitude</th>\n","      <th>Longitude</th>\n","      <th>tas</th>\n","      <th>pr</th>\n","      <th>huss</th>\n","      <th>rlds</th>\n","      <th>rsds</th>\n","      <th>sfcWind</th>\n","      <th>ndsi1</th>\n","      <th>...</th>\n","      <th>Latitude_1</th>\n","      <th>Latitude_2</th>\n","      <th>Latitude_3</th>\n","      <th>Latitude_4</th>\n","      <th>Longitude_0</th>\n","      <th>Longitude_1</th>\n","      <th>Longitude_2</th>\n","      <th>Longitude_3</th>\n","      <th>Longitude_4</th>\n","      <th>Longitude_5</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2001-01-01 12:00:00</td>\n","      <td>46.13</td>\n","      <td>10.18</td>\n","      <td>263.203278</td>\n","      <td>1.564806e-09</td>\n","      <td>0.001066</td>\n","      <td>213.009583</td>\n","      <td>83.646156</td>\n","      <td>2.344069</td>\n","      <td>62.501556</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2001-01-01 12:00:00</td>\n","      <td>46.13</td>\n","      <td>10.34</td>\n","      <td>269.678467</td>\n","      <td>2.030947e-08</td>\n","      <td>0.001068</td>\n","      <td>211.131866</td>\n","      <td>80.710426</td>\n","      <td>1.355827</td>\n","      <td>61.311447</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2001-01-01 12:00:00</td>\n","      <td>46.13</td>\n","      <td>10.50</td>\n","      <td>256.047241</td>\n","      <td>1.650252e-08</td>\n","      <td>0.000851</td>\n","      <td>199.728699</td>\n","      <td>79.973618</td>\n","      <td>1.899092</td>\n","      <td>65.980927</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2001-01-01 12:00:00</td>\n","      <td>46.13</td>\n","      <td>10.66</td>\n","      <td>260.295776</td>\n","      <td>7.775169e-11</td>\n","      <td>0.001006</td>\n","      <td>202.456894</td>\n","      <td>81.029640</td>\n","      <td>3.666345</td>\n","      <td>68.090210</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2001-01-01 12:00:00</td>\n","      <td>46.13</td>\n","      <td>10.82</td>\n","      <td>266.777985</td>\n","      <td>2.028565e-07</td>\n","      <td>0.001364</td>\n","      <td>213.373550</td>\n","      <td>77.965660</td>\n","      <td>2.358261</td>\n","      <td>53.926231</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>486291</th>\n","      <td>2018-12-31 12:00:00</td>\n","      <td>46.69</td>\n","      <td>10.34</td>\n","      <td>263.931519</td>\n","      <td>2.894347e-06</td>\n","      <td>0.002223</td>\n","      <td>222.105026</td>\n","      <td>82.042076</td>\n","      <td>2.121476</td>\n","      <td>50.924480</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>486294</th>\n","      <td>2018-12-31 12:00:00</td>\n","      <td>46.69</td>\n","      <td>10.82</td>\n","      <td>267.040100</td>\n","      <td>2.160477e-05</td>\n","      <td>0.002353</td>\n","      <td>205.163071</td>\n","      <td>81.809784</td>\n","      <td>3.399118</td>\n","      <td>52.055504</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>486297</th>\n","      <td>2018-12-31 12:00:00</td>\n","      <td>46.69</td>\n","      <td>11.31</td>\n","      <td>270.152191</td>\n","      <td>7.662312e-06</td>\n","      <td>0.003096</td>\n","      <td>218.106552</td>\n","      <td>79.163933</td>\n","      <td>4.733071</td>\n","      <td>43.464687</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>486298</th>\n","      <td>2018-12-31 12:00:00</td>\n","      <td>46.69</td>\n","      <td>11.47</td>\n","      <td>267.166138</td>\n","      <td>1.842953e-05</td>\n","      <td>0.002798</td>\n","      <td>232.383804</td>\n","      <td>71.881706</td>\n","      <td>2.661735</td>\n","      <td>37.354782</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>486299</th>\n","      <td>2018-12-31 12:00:00</td>\n","      <td>46.69</td>\n","      <td>11.64</td>\n","      <td>273.776550</td>\n","      <td>9.896937e-06</td>\n","      <td>0.003227</td>\n","      <td>236.890198</td>\n","      <td>77.595032</td>\n","      <td>1.531617</td>\n","      <td>15.043457</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>390708 rows × 25 columns</p>\n","</div>"],"text/plain":["                      Date  Latitude  Longitude         tas            pr  \\\n","0      2001-01-01 12:00:00     46.13      10.18  263.203278  1.564806e-09   \n","1      2001-01-01 12:00:00     46.13      10.34  269.678467  2.030947e-08   \n","2      2001-01-01 12:00:00     46.13      10.50  256.047241  1.650252e-08   \n","3      2001-01-01 12:00:00     46.13      10.66  260.295776  7.775169e-11   \n","4      2001-01-01 12:00:00     46.13      10.82  266.777985  2.028565e-07   \n","...                    ...       ...        ...         ...           ...   \n","486291 2018-12-31 12:00:00     46.69      10.34  263.931519  2.894347e-06   \n","486294 2018-12-31 12:00:00     46.69      10.82  267.040100  2.160477e-05   \n","486297 2018-12-31 12:00:00     46.69      11.31  270.152191  7.662312e-06   \n","486298 2018-12-31 12:00:00     46.69      11.47  267.166138  1.842953e-05   \n","486299 2018-12-31 12:00:00     46.69      11.64  273.776550  9.896937e-06   \n","\n","            huss        rlds       rsds   sfcWind      ndsi1  ...  Latitude_1  \\\n","0       0.001066  213.009583  83.646156  2.344069  62.501556  ...           0   \n","1       0.001068  211.131866  80.710426  1.355827  61.311447  ...           0   \n","2       0.000851  199.728699  79.973618  1.899092  65.980927  ...           0   \n","3       0.001006  202.456894  81.029640  3.666345  68.090210  ...           0   \n","4       0.001364  213.373550  77.965660  2.358261  53.926231  ...           0   \n","...          ...         ...        ...       ...        ...  ...         ...   \n","486291  0.002223  222.105026  82.042076  2.121476  50.924480  ...           0   \n","486294  0.002353  205.163071  81.809784  3.399118  52.055504  ...           0   \n","486297  0.003096  218.106552  79.163933  4.733071  43.464687  ...           0   \n","486298  0.002798  232.383804  71.881706  2.661735  37.354782  ...           0   \n","486299  0.003227  236.890198  77.595032  1.531617  15.043457  ...           0   \n","\n","        Latitude_2  Latitude_3  Latitude_4  Longitude_0  Longitude_1  \\\n","0                0           0           0            1            0   \n","1                0           0           0            1            0   \n","2                0           0           0            1            0   \n","3                0           0           0            0            1   \n","4                0           0           0            0            1   \n","...            ...         ...         ...          ...          ...   \n","486291           1           0           0            1            0   \n","486294           1           0           0            0            1   \n","486297           1           0           0            0            0   \n","486298           1           0           0            0            0   \n","486299           1           0           0            0            0   \n","\n","        Longitude_2  Longitude_3  Longitude_4  Longitude_5  \n","0                 0            0            0            0  \n","1                 0            0            0            0  \n","2                 0            0            0            0  \n","3                 0            0            0            0  \n","4                 0            0            0            0  \n","...             ...          ...          ...          ...  \n","486291            0            0            0            0  \n","486294            0            0            0            0  \n","486297            0            1            0            0  \n","486298            0            1            0            0  \n","486299            0            0            1            0  \n","\n","[390708 rows x 25 columns]"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["#bookmark\n","print(df.columns)\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["df[\"ndsi\"] = df[\"ndsi1\"]\n","df= df.drop([\"Month\", \"week_number\", \"ndsi1\"],axis =1)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["parameter_array = [\"tas\", \"pr\", \"huss\", \"rlds\",  \"rsds\", \"sfcWind\"]\n","min_max_df = pd.read_csv(\"./min_max_of_all_parameters.csv\")\n","for parameter in parameter_array:\n","    min_val, max_val =  get_min_max(min_max_df, parameter)\n","    df[parameter] = (df[parameter] - min_val) / (max_val - min_val)\n","\n","df['ndsi'] = df['ndsi'].apply(lambda x: x / 100 if not pd.isnull(x) else x)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# percentage_array= []\n","# actual_total_zeroes_array = []\n","# desired_zero_array = [25000]  # Desired number of zero values\n","# b = 0.01  # Initial percentage to sample\n","\n","# for a in desired_zero_array:\n","#     a = a - 2500 \n","#     tolerance = 0.15 * a \n","\n","#     df[\"Month\"] = df['Date'].dt.month\n","#     df_to_delete = df\n","#     df = df[df['Month'].isin([1, 2, 3, 12])]\n","\n","#     logger.info(f\"Tolerance:  {tolerance}\")\n","#     logger.info(f\"Final range of near zeores: {a +2500 - tolerance} - {a + 2500 + tolerance}\")\n","\n","#     while True:\n","#         zero_values_array = []\n","\n","#         # Step 2: Iterate through each month\n","#         for month in range(4, 12):  # Months from April to November\n","#             # Step 3: Sample rows and calculate the number of near zero values\n","#             sampled_data = df_to_delete[(df_to_delete['Date'].dt.month == month)].sample(frac=b)\n","#             zero_values_array.append(np.sum((sampled_data['ndsi'] >= -0.005) & (sampled_data['ndsi'] <= 0.005)))\n","\n","#         # Step 4: Check the sum of zero values\n","#         total_zero_values = sum(zero_values_array)\n","\n","#         # Step 5: Adjust the percentage (b)\n","#         if total_zero_values > a + tolerance:\n","#             b -= 0.01\n","#         elif total_zero_values < a - tolerance:\n","#             b += 0.01\n","#         else:\n","#             break  # Convergence reached\n","\n","#     # Step 6: Display the final value of b\n","#     logger.info(f\"Final value of b: {b}\")\n","#     logger.info(f\"Total zero values: {total_zero_values+2500}\")\n","#     percentage_array.append(b)\n","#     actual_total_zeroes_array.append(total_zero_values+2500)\n","\n","\n","#     # Now you can use this final value of b to sample your DataFrame\n","#     final_sampled_data = pd.concat([df_to_delete[(df_to_delete['Date'].dt.month == month)].sample(frac=b) for month in range(4, 12)])\n","#     df = pd.concat([final_sampled_data, df], ignore_index=False)\n","#     df = df.sort_index()\n","\n","#     # the old code for the sampling has been moved to less_imp/rough"]},{"cell_type":"markdown","metadata":{},"source":["### Plotting of per month data (should be kept folded to keep everything compact)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# import plotly.express as px\n","# from plotly.subplots import make_subplots\n","# import plotly.graph_objs as go\n","\n","# # Plot histogram for df\n","# fig1 = px.histogram(df, x='ndsi', nbins=100)\n","# fig1.update_layout(\n","#     xaxis_title='Value',\n","#     yaxis_title='Frequency',\n","# )\n","\n","# # Plot histogram for df_to_delete\n","# fig2 = px.histogram(df_to_delete, x='ndsi', nbins=100)\n","# fig2.update_layout(\n","#     xaxis_title='Value',\n","#     yaxis_title='Frequency',\n","# )\n","\n","# # Create subplots with two columns\n","# fig = make_subplots(rows=1, cols=2, subplot_titles=('Before', 'After'))\n","# fig.add_trace(fig1.data[0], row=1, col=2)\n","# fig.add_trace(fig2.data[0], row=1, col=1)\n","\n","# fig.update_layout(\n","#     title='Before and after zero sampling in mid-year months',\n","#     width=1000,  # Total width of the combined plots\n","#     height=400,  # Height of the combined plots\n","# )\n","\n","# plotly_image_path = os.path.join(folder_name, 'before_after_zero_sampling.png')\n","# fig.write_image(plotly_image_path)\n","\n","\n","# fig.show()\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'df_to_delete' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[25], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming 'df_to_delete' is your DataFrame\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Convert 'Date' column to datetime\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m df_to_delete[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mdf_to_delete\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Extract month from the 'Date' column\u001b[39;00m\n\u001b[0;32m      9\u001b[0m df_to_delete[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_to_delete[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mmonth\n","\u001b[1;31mNameError\u001b[0m: name 'df_to_delete' is not defined"]}],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Assuming 'df_to_delete' is your DataFrame\n","# Convert 'Date' column to datetime\n","df_to_delete['Date'] = pd.to_datetime(df_to_delete['Date'])\n","\n","# Extract month from the 'Date' column\n","df_to_delete['Month'] = df_to_delete['Date'].dt.month\n","df['Month'] = df['Date'].dt.month\n","\n","# Create a new figure and subplots\n","fig, axs = plt.subplots(4, 6, figsize=(20, 10))  # 4 rows, 8 columns for 12 months for both DataFrames\n","\n","\n","# row = (month - 1) // 3  # Calculate the row index for the subplot\n","#     col = (month - 1) % 3\n","\n","# Plot histograms for each month in df\n","for month in range(1, 13):\n","    row = (month - 1) // 3  # Calculate the row index for the subplot\n","    col = (month - 1) % 3   # Calculate the column index for the subplot\n","    \n","    # Filter data for the current month in df_to_delete\n","    month_data_df_to_delete = df_to_delete[df_to_delete['Month'] == month]['ndsi']\n","    \n","    # Plot histogram in the corresponding subplot for df_to_delete\n","    axs[row, col].hist(month_data_df_to_delete, bins=20, color='salmon', alpha=0.7)\n","    axs[row, col].set_title(f'Month {month} (before)')\n","    axs[row, col].set_xlabel('NDSI')\n","    axs[row, col].set_ylabel('Frequency')\n","\n","\n","# Plot histograms for each month in df_to_delete on the right\n","for month in range(1, 13):\n","    row = (month - 1) // 3  # Calculate the row index for the subplot\n","    col = (month - 1) % 3 + 3  # Shift to the right by 4 columns\n","    \n","    # Filter data for the current month in df\n","    month_data_df = df[df['Month'] == month]['ndsi']\n","    \n","    # Plot histogram in the corresponding subplot for df\n","    axs[row, col].hist(month_data_df, bins=20, color='skyblue', alpha=0.7)\n","    axs[row, col].set_title(f'Month {month} (after) | Fraction = {month_fraction_array_padded[month-1]}')\n","    axs[row, col].set_xlabel('NDSI')\n","    axs[row, col].set_ylabel('Frequency')\n","    \n","    \n","# # Hide empty subplots\n","# for i in range(12, 16):\n","#     fig.delaxes(axs[3, i])\n","\n","df = df.drop([\"Month\"], axis=1)\n","# Adjust layout and display the subplots\n","plt.tight_layout()\n","\n","matplotlib_image_path = os.path.join(folder_name, 'each_month_zero_cleaning.png')\n","plt.savefig(matplotlib_image_path)\n","\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Data splitting"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47,"status":"ok","timestamp":1700686397725,"user":{"displayName":"Muhammad Arhab","userId":"05674263421856131577"},"user_tz":-330},"id":"MtXlANTgmIdT","outputId":"507d06bf-1d05-442d-f7e4-5c564e04a966"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-01-18 13:01:04,729 - INFO - Number of rows: 3907/390708\n"]},{"data":{"text/plain":["3907"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# percentage = 100\n","number_of_rows = int(percentage/100*len(df))\n","logger.info(f\"Number of rows: {number_of_rows}/{len(df)}\")\n","number_of_rows"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":543,"status":"ok","timestamp":1700686405589,"user":{"displayName":"Muhammad Arhab","userId":"05674263421856131577"},"user_tz":-330},"id":"A8SgbC3JmIdT","outputId":"3d107652-3e02-44e0-b5e3-5de797c4d2f5"},"outputs":[],"source":["df_reduced = df.sample(frac=percentage/100)\n","df_reduced = df_reduced.sort_index()"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"tBK6IUzVmIdT"},"outputs":[],"source":["ultimate_test= df_reduced[df_reduced['Date'].dt.year > 2017]\n","df_reduced = df_reduced.drop(ultimate_test.index)\n","df_reduced = df_reduced.sort_index()"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"UxtG3w8NmIdT"},"outputs":[],"source":["test = df_reduced.sample(frac=0.2)\n","test = test.sort_index()\n","\n","train = df_reduced.drop(test.index)\n","train = train.sort_index()"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"trNjmOgtmIdT"},"outputs":[],"source":["# X = df.iloc[:number_of_rows, 1:-1].values\n","# y = df.iloc[:number_of_rows, -1].values\n","\n","# Step 7: Split Data\n","X_train, X_test = train.drop([\"Date\", \"ndsi\", \"Latitude\", \"Longitude\"], axis=1).values, test.drop([\"Date\", \"ndsi\", \"Latitude\", \"Longitude\"], axis=1).values\n","y_train, y_test = train[\"ndsi\"].values, test[\"ndsi\"].values"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"GlmYB88FmIdU"},"outputs":[],"source":["X_ultimate_test, y_ultimate_test = ultimate_test.drop([\"Date\", \"ndsi\"], axis=1).values, ultimate_test[\"ndsi\"].values"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-01-18 13:01:07,952 - INFO - y_train shape is: (2946, 1)\n","2024-01-18 13:01:07,953 - INFO - X_train shape is: (2946, 1, 19)\n","2024-01-18 13:01:07,955 - INFO - y_test shape is: (737, 1)\n","2024-01-18 13:01:07,956 - INFO - X_test shape is: (737, 1, 19)\n"]}],"source":["temp_shape = X_train.shape[1]\n","# Extracting features and labels\n","X_train = X_train.reshape(-1, 1, temp_shape)  # Reshaping to (146246, 1, 32)\n","y_train = y_train.reshape(-1, 1)  # Assuming the label is in column 35\n","\n","X_test = X_test.reshape(-1, 1, temp_shape)  # Reshaping to (146246, 1, 32)\n","y_test = y_test.reshape(-1, 1)  # Assuming the label is in column 35\n","\n","\n","# logger.infoing shapes\n","logger.info(f\"y_train shape is: {y_train.shape}\")\n","logger.info(f\"X_train shape is: {X_train.shape}\")\n","\n","logger.info(f\"y_test shape is: {y_test.shape}\")\n","logger.info(f\"X_test shape is: {X_test.shape}\")\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["# %run \"conversion_to_LSTM_format.ipynb\""]},{"cell_type":"markdown","metadata":{},"source":["# Model Architecture"]},{"cell_type":"markdown","metadata":{},"source":["## Machine learning stuff, mainly folded"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"dddH0F-omIdU"},"outputs":[],"source":["if model_choice==\"ml\":\n","    # Step 8: Train Random Forest Model\n","    rf_model = RandomForestRegressor(n_estimators=100)\n","\n","    start_time = time.time()\n","    rf_model.fit(X_train, y_train)\n","    end_time = time.time()\n","    execution_time_fitting = end_time - start_time\n","    logger.info(f\"Time taken to fit {execution_time_fitting/60} min\")\n","    logger.info(\"Fitting done\")\n","\n","    # Step 9: Model historical\n","    start_time = time.time()\n","    y_pred = rf_model.predict(X_test)\n","    end_time = time.time()\n","    y_ultimate_pred = rf_model.predict(X_ultimate_test)\n","    execution_time_prediciting = end_time - start_time\n","\n","    logger.info(f\"Time taken to predict {execution_time_prediciting}\")\n"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["# # Define the model\n","#     model = Sequential()\n","#     # model.add(LSTM(128, input_shape=(1,32), activation='relu'))\n","#     model.add(LSTM(units=64, input_shape=(1,7), return_sequences=False, activation='relu'))\n","#     # model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))  # Increased neurons\n","#     # model.add(Dropout(0.5))\n","#     # # model.add(Dense(128, activation='relu'))\n","#     # # model.add(Dropout(0.5))\n","#     # model.add(Dense(64, activation='relu'))\n","#     # model.add(Dropout(0.5))\n","#     # model.add(Dense(32, activation='relu'))\n","#     # model.add(Dropout(0.5))\n","#     model.add(Dense(1, activation='linear'))  # Assuming NDSI is a continuous variable"]},{"cell_type":"markdown","metadata":{},"source":["## Deep learning stuff, the main thing "]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":995658,"status":"ok","timestamp":1700687406667,"user":{"displayName":"Muhammad Arhab","userId":"05674263421856131577"},"user_tz":-330},"id":"5uUDTw6PmIdU","outputId":"364413df-9e4a-4d9f-c3e9-4b486090f92a"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-01-18 13:01:14,209 - INFO - GPU found. Running on GPU.\n"]}],"source":["if model_choice==\"tf\":\n","    import tensorflow as tf\n","    from tensorflow.keras.models import Sequential\n","    from tensorflow.keras.layers import Dense, Dropout, LSTM, SimpleRNN\n","    from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","    from tensorflow.keras import optimizers\n","\n","    # Check for GPU availability\n","    if tf.config.list_physical_devices('GPU'):\n","        logger.info('GPU found. Running on GPU.')\n","    else:\n","        logger.info('No GPU found. Running on CPU.')\n","\n","    # Specify GPU device if available\n","    physical_devices = tf.config.list_physical_devices('GPU')\n","    if len(physical_devices) > 0:\n","        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n","\n","    # Convert data to TensorFlow tensors\n","    X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n","    y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n","    X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)\n","    y_test_tensor = tf.convert_to_tensor(y_test, dtype=tf.float32)\n","    \n"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]},{"name":"stderr","output_type":"stream","text":["2024-01-18 13:01:29,676 - INFO - Mean absolute Error on Test Set: 0.12213493883609772\n"]},{"name":"stdout","output_type":"stream","text":["24/24 [==============================] - 2s 7ms/step\n"]}],"source":["if model_choice==\"tf\" and run ==0:\n","    # Define the model\n","    model = Sequential()\n","    model.add(LSTM(units=180, input_shape=(1,temp_shape), return_sequences=True, activation='relu'))\n","    model.add(LSTM(50, return_sequences=True))\n","    model.add(LSTM(10, return_sequences=True))\n","    model.add(LSTM(5, return_sequences=False))\n","    model.add(Dense(10, activation='relu'))\n","    model.add(Dense(1, activation='linear'))  # Assuming NDSI is a continuous variable\n","\n","    # Compile the model\n","    model.compile(optimizer=\"adam\", loss='mean_squared_error', metrics=['mae'])\n","\n","    # filepath = 'models/{epoch:02d}-{loss:.4f}-{val_loss:.4f}-{mae:.4f}-{val_mae:.4f}.hdf5'\n","    callbacks = [EarlyStopping(monitor='val_loss', patience=20)]\n","    # ModelCheckpoint(filepath, monitor='loss', save_best_only=True, mode='min')\n","\n","    with open(log_file, 'a') as f:\n","        with stdout_redirected(f):\n","            model.summary()\n","            \n","            # Train the model on GPU\n","            start_time = time.time()\n","            history = model.fit(X_train_tensor, y_train_tensor, epochs=epochs, batch_size=batch_size, callbacks=callbacks, validation_split=0.2)\n","            end_time = time.time()\n","            execution_time_fitting = end_time - start_time\n","\n","            # Evaluate the model on GPU\n","            loss, mae = model.evaluate(X_test_tensor, y_test_tensor)\n","            logger.info(f'Mean absolute Error on Test Set: {mae}')\n","\n","    # Make predictions on GPU\n","    y_pred = model.predict(X_test_tensor)\n","\n","    remove_eta_lines(run_number)\n","    model_save_path = os.path.join(folder_name, 'model.h5')\n","    model.save(model_save_path)"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["if model_choice==\"tf\" and run ==1:\n","    # Define a function to build the model with hyperparameters\n","    def build_model(hp):\n","        model = Sequential()\n","        model.add(LSTM(units=hp.Int('units_1', min_value=50, max_value=200, step=10), input_shape=(1, temp_shape), return_sequences=True, activation='relu'))\n","        model.add(LSTM(units=hp.Int('units_2', min_value=20, max_value=100, step=10), return_sequences=True))\n","        model.add(LSTM(units=hp.Int('units_3', min_value=10, max_value=50, step=10), return_sequences=True))\n","        model.add(LSTM(units=hp.Int('units_4', min_value=5, max_value=30, step=5), return_sequences=False))\n","        model.add(Dense(units=hp.Int('units_5', min_value=10, max_value=100, step=10), activation='relu'))\n","        model.add(Dense(1, activation='linear'))\n","\n","        optimizer_choice = hp.Choice('optimizer', ['adam', 'rmsprop', 'sgd'])  # Define optimizers for regression\n","        if optimizer_choice == 'adam':\n","            optimizer = optimizers.Adam(learning_rate=hp.Choice('learning_rate_adam', values=[1e-2, 1e-3, 1e-4]))\n","        elif optimizer_choice == 'rmsprop':\n","            optimizer = optimizers.RMSprop(learning_rate=hp.Choice('learning_rate_rmsprop', values=[1e-2, 1e-3, 1e-4]))\n","        else:\n","            optimizer = optimizers.SGD(learning_rate=hp.Choice('learning_rate_sgd', values=[1e-2, 1e-3, 1e-4]))\n","\n","        model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n","        return model\n","\n","    # Instantiate the tuner for hyperparameter search\n","    tuner = RandomSearch(\n","        build_model,\n","        objective='val_loss',\n","        max_trials=3,  # Number of hyperparameter combinations to try\n","        executions_per_trial=1,\n","        directory=folder_name,  # Directory to store the results\n","        project_name='regression_optimizers_tuner_2'\n","    )\n","\n","    with open(log_file, 'a') as f:\n","        with stdout_redirected(f):\n","            # Search for the best hyperparameter configuration\n","            tuner.search(X_train_tensor, y_train_tensor, epochs=50, validation_split=0.2)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#bookmark\n","if model_choice==\"tf\" and run ==1:\n","    # Get the best hyperparameters and build the final model\n","    best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n","    best_model = tuner.hypermodel.build(best_hp)\n","\n","    # Train the best model\n","    history = best_model.fit(X_train_tensor, y_train_tensor, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[EarlyStopping(patience=20)])\n","\n","    # Evaluate the best model\n","    loss, mae = best_model.evaluate(X_test_tensor, y_test_tensor)\n","    logger.info(f'Mean absolute Error on Test Set: {mae}')\n","\n","    # Make predictions using the best model\n","    y_pred = best_model.predict(X_test_tensor)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from keras.models import load_model\n","if model_choice==\"tf\" and run ==2:\n","    # Provide the directory path where checkpoint files or model information is stored\n","\n","    # Load the model\n","    loaded_model = load_model(rf'D:\\thesis_data\\notebooks\\model_runs\\run_{load_run_number}\\model.h5')\n","\n","    # Train the best model (assuming X_train and y_train are defined)\n","    # history = loaded_model.fit(X_train_tensor, y_train_tensor, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[EarlyStopping(patience=20)])\n","\n","    # Evaluate the best model\n","    loss, mae = loaded_model.evaluate(X_test_tensor, y_test_tensor)\n","    logger.info(f'Mean absolute Error on Test Set: {mae}')\n","\n","    # Make predictions using the best model\n","    y_pred = loaded_model.predict(X_test_tensor)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"onv4DFogEnhl"},"outputs":[],"source":["if model_choice==\"tf\":\n","    # Plot the training and validation loss\n","    plt.plot(history.history['loss'], label='Training Loss')\n","    plt.plot(history.history['val_loss'], label='Validation Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Mean Squared Error')\n","    plt.legend()\n","    \n","    matplotlib_image_path = os.path.join(folder_name, 'loss_vs_epoch_graph.png')\n","    plt.savefig(matplotlib_image_path)\n","    \n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YR9TjTZ3mIdV"},"outputs":[],"source":["import math\n","from sklearn.metrics import  mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import r2_score\n","\n","if model_choice == \"tf\":\n","    y_pred = y_pred.flatten()\n","    y_test = y_test.flatten()\n","\n","\n","mae = mean_absolute_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","# nse_value = nse(y_pred, y_test)\n","\n","mse_value = mean_squared_error(y_test,y_pred)\n","rmse_value = math.sqrt(mse_value)\n","n_rmse_value = calculate_n_rmse(y_test, y_pred)\n","\n","pbias_value = pbias(y_test, y_pred)\n","\n","temp_percentage = round(number_of_rows*100/len(df),2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wjd1nuVHmIdV","outputId":"72861d07-0b91-46dd-dd92-1b16af33aa17"},"outputs":[],"source":["logger.info(f\"{number_of_rows}/{len(df)} = {temp_percentage} % \\n\")\n","# logger.info(f\"Time taken to fit {execution_time_fitting/60} min\")\n","# logger.info(f\"NSE value: {nse_value}\")\n","logger.info(f'R2 Score: {r2}')\n","# logger.info(f'Self defined R2 Score: {self_defined_r2} \\n')\n","logger.info(f\"P-bias value: {pbias_value} \\n\")\n","\n","# logger.info(f\"MSE value: {mse_value}\")\n","logger.info(f\"RMSE value: {rmse_value}\")\n","logger.info(f\"N-RMSE value: {n_rmse_value}\")"]},{"cell_type":"markdown","metadata":{"notebookRunGroups":{"groupValue":"1"}},"source":["# Plotting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZXAMMobTmIdV","outputId":"b90b9f7b-acad-469b-f15f-2016dbe59e05"},"outputs":[],"source":["# y_pred = y_pred.flatten()\n","comparison_df = pd.DataFrame({\n","    \"Date\" : test[\"Date\"].values,\n","    \"Latitude\" : test[\"Latitude\"].values,\n","    \"Longitude\" : test[\"Longitude\"].values,\n","    \"y_test\": y_test,\n","    \"y_pred\": y_pred.flatten()\n","    })\n","comparison_df['Date'] = pd.to_datetime(comparison_df['Date'])\n","\n","df_image_path = os.path.join(folder_name, 'test_vs_predicted.csv')\n","comparison_df.to_csv(df_image_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["comparison_df, scale = comparison_df.sort_values(by='Date'), \"Original without mean\"\n","fig = px.scatter(comparison_df, x=\"Date\" , y=[\"y_pred\",\"y_test\"])\n","fig.update_layout(title=f'Predicted vs Observed | {scale}', width = 1800, height = 400)\n","\n","plotly_image_path = os.path.join(folder_name, 'Original_without_mean.png')\n","fig.write_image(plotly_image_path)\n","\n","fig"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["comparison_df_1 = comparison_df[(comparison_df['Latitude'] ==  46.58) & (comparison_df['Longitude'] == 11.15)]\n","comparison_df_1, scale = comparison_df_1.sort_values(by='Date'), \"Centre without mean\"\n","fig = px.scatter(comparison_df, x=\"Date\" , y=[\"y_pred\",\"y_test\"])\n","fig.update_layout(title=f'Predicted vs Observed | {scale}', width = 1800, height = 400)\n","\n","plotly_image_path = os.path.join(folder_name, 'Centre_without_mean.png')\n","fig.write_image(plotly_image_path)\n","\n","fig"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["comparison_df['Date'], scale = comparison_df['Date'].dt.strftime('%d-%m-%Y'), \"Average Daily\"\n","comparison_df= comparison_df.groupby('Date')[[\"y_test\", \"y_pred\"]].mean().reset_index()\n","comparison_df['Date'] = pd.to_datetime(comparison_df['Date'], format='%d-%m-%Y')\n","comparison_df = comparison_df.sort_values(by='Date')\n","# comparison_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = px.scatter(comparison_df, x=\"Date\" , y=[\"y_pred\",\"y_test\"])\n","fig.update_layout(title=f'Predicted vs Observed | {scale}', width = 1800, height = 400)\n","\n","plotly_image_path = os.path.join(folder_name, 'Average daily.png')\n","fig.write_image(plotly_image_path)\n","fig"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = px.line(comparison_df, x=\"Date\" , y=[\"y_pred\",\"y_test\"])\n","fig.update_layout(title=f'Predicted vs Observed | {scale}', width = 1800, height = 400)\n","\n","plotly_image_path = os.path.join(folder_name, 'Average daily (line).png')\n","fig.write_image(plotly_image_path)\n","fig"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["comparison_df['Date'], scale = comparison_df['Date'].dt.strftime('%m-%Y'), \"Average monthly\"\n","comparison_df= comparison_df.groupby('Date')[[\"y_test\", \"y_pred\"]].mean().reset_index()\n","comparison_df['Date'] = pd.to_datetime(comparison_df['Date'])\n","comparison_df = comparison_df.sort_values(by='Date')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n3TheIJxmIdV","outputId":"4d72860f-2b71-428a-8581-fd4ddcdeee1e"},"outputs":[],"source":["fig = px.line(comparison_df, x=\"Date\" , y=[\"y_pred\",\"y_test\"])\n","fig.update_layout(title=f'Predicted vs Observed | {scale}', width = 1800, height = 400)\n","\n","\n","plotly_image_path = os.path.join(folder_name, 'Average monthly.png')\n","fig.write_image(plotly_image_path)\n","\n","fig"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xOBTq-QymIdV","outputId":"1e576011-d1c5-49e9-b869-d256c6adab85"},"outputs":[],"source":["# # for ultimate testing\n","\n","# y_ultimate_pred = y_ultimate_pred.flatten()\n","# comparison_df = pd.DataFrame({\n","#     \"Date\" : ultimate_test[\"Date\"].values,\n","#     \"y_ultimate_test\": y_ultimate_test,\n","#     \"y_ultimate_pred\": y_ultimate_pred\n","#     })\n","# # comparison_df['Date'] = pd.to_datetime(comparison_df['Date'])\n","# # comparison_df['Date'] = comparison_df['Date'].dt.strftime('%m-%Y')\n","\n","\n","# comparison_df= comparison_df.groupby('Date')[[\"y_ultimate_test\", \"y_ultimate_pred\"]].mean().reset_index()\n","# comparison_df['Date'] = pd.to_datetime(comparison_df['Date'])\n","# comparison_df = comparison_df.sort_values(by='Date')\n","# comparison_df\n","\n","\n","# fig = px.line(comparison_df, x=\"Date\" , y=[\"y_ultimate_pred\",\"y_ultimate_test\"])\n","# fig.update_layout(title=f'Predicted vs Observed')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"xarray-tutorial","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
